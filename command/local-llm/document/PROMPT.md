I’m planning to choose an LLM (Large Language Model) to run locally on my MacBook Pro M3 Max. There are several potential use cases I want to explore, such as:
* Periodic background tasks (e.g., a cron job) that check specific files and act based on a natural language prompt
* Tasks requiring light reasoning, like scoring coding agents’ outputs with explicit numeric evaluations
* Querying an MCP server to retrieve conversation histories of coding agents (e.g., Claude Code sessions) and helping users leverage that information

While remote LLMs like Claude, ChatGPT, and Gemini are generally more capable, they come with limitations such as API costs, rate limits, and dependency on network stability. Building a reliable and manageable local system could offer better performance and autonomy for certain workflows.

To move forward, I’d like to explore the following:
* Model selection: Research candidate models suitable for local use
* Model management: Tools like Ollama, LM Studio, or custom CLI-based solutions
* Inference engines: Options that enable faster and more intelligent execution

I’d appreciate your help with this research. I plan to evaluate each option through hands-on testing with my actual use cases, so please provide multiple viable choices along with relevant details to support informed decision-making.
